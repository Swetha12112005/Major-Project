# -*- coding: utf-8 -*-
"""Student Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1YJPI2VyONauWcUFvwzXxSS_niIDbdCyC

## 1. Data Loading and Initial Inspection

- Import core Python libraries for data analysis and visualization (pandas, numpy, matplotlib, seaborn).
- Load the `student_placement_xgboost_dataset.xlsx` file into a pandas DataFrame.
- Verify the shape of the dataset (rows vs columns) to understand data volume.
- Preview a few rows to confirm column names, data types, and that the file has been read correctly.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

plt.style.use("seaborn-v0_8")
sns.set_palette("Set2")

file_path = "/content/student_placement_xgboost_dataset.xlsx"
df = pd.read_excel(file_path)

print("Shape (rows, columns):", df.shape)
df.head()

"""## 2. Data Dictionary and Structural Overview

- List all column names to understand the features available in the dataset.
- Check data types and non-null counts to detect potential issues (wrong types, missing values).
- Generate descriptive statistics for numeric columns (mean, std, min, max, quartiles).
- Use this step to build a mental data dictionary for further analysis and modeling.

"""

# Print all column names to understand available features.
print("Columns:", df.columns.tolist())

# info() helps us see data types and non-null counts.
# This is useful for detecting wrong dtypes or missing values.
df.info()

# Describe numeric columns: count, mean, std, min, max, quartiles.
df.describe().T

"""## 3. Data Quality Assessment

- Identify missing values in each column to decide if imputation or dropping is needed.
- Detect duplicate records to avoid bias and data leakage in the model.
- Examine numeric columns via boxplots to spot extreme outliers.
- Use these findings to plan cleaning steps before model building.

"""

# ======================================================
# SECTION 3: DATA QUALITY CHECKS (MISSING & DUPLICATES)
# ======================================================

# Check how many missing values exist in each column.
missing = df.isna().sum().sort_values(ascending=False)
print("Missing values per column:\n", missing)

# Check duplicate rows to avoid data leakage or bias.
duplicate_count = df.duplicated().sum()
print("Number of duplicate rows:", duplicate_count)

# Optionally inspect the duplicated rows if any are present.
if duplicate_count > 0:
    df[df.duplicated()]

# ======================================================
# SECTION 3.2: OUTLIER SCAN FOR NUMERIC VARIABLES
# ======================================================

# Identify numeric columns for further analysis.
numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
print("Numeric columns:", numeric_cols)

# Draw boxplots for each numeric column to visually inspect outliers.
fig, axes = plt.subplots(nrows=len(numeric_cols), ncols=1, figsize=(8, 3*len(numeric_cols)))

# Handle case when there's only one numeric column (axes is not a list).
if len(numeric_cols) == 1:
    axes = [axes]

for ax, col in zip(axes, numeric_cols):
    sns.boxplot(x=df[col], ax=ax)
    ax.set_title(f"Boxplot of {col}")

plt.tight_layout()
plt.show()

"""4. Univariate analysis"""

# =========================================
# SECTION 4: UNIVARIATE FEATURE ANALYSIS
# =========================================

# Separate categorical and numeric columns for targeted analysis.
categorical_cols = df.select_dtypes(include=["object"]).columns.tolist()
print("Categorical columns:", categorical_cols)
print("Numeric columns:", numeric_cols)

# ------------------------------
# 4.1 TARGET DISTRIBUTION
# ------------------------------

# Visualize how many students are placed vs not placed.
plt.figure(figsize=(5,4))
sns.countplot(x="placed", data=df)
plt.title("Placement Status Distribution")
plt.xlabel("Placed (1 = Yes, 0 = No)")
plt.ylabel("Count")
plt.show()

# Show percentage share of each class (0 / 1).
(df["placed"].value_counts(normalize=True) * 100).round(2)

# ------------------------------
# 4.2 CATEGORICAL DISTRIBUTIONS
# ------------------------------

# For each categorical feature (department, skill_stack, etc.),
# visualize its distribution to see how many students fall into each category.
for col in categorical_cols:
    plt.figure(figsize=(6,4))
    order = df[col].value_counts().index  # sort categories by frequency
    sns.countplot(x=col, data=df, order=order)
    plt.title(f"Distribution of {col}")
    plt.xticks(rotation=45)
    plt.ylabel("Count")
    plt.tight_layout()
    plt.show()

    # Also print raw counts for quick reference.
    print(f"\nValue counts for {col}:")
    print(df[col].value_counts())

# ------------------------------
# 4.3 NUMERIC DISTRIBUTIONS
# ------------------------------

# Quick overview histograms for all numeric features.
df[numeric_cols].hist(figsize=(12, 10), bins=20)
plt.suptitle("Numeric Features Distribution", y=1.02)
plt.tight_layout()
plt.show()

# More detailed distribution (histogram + KDE) for each numeric variable.
for col in numeric_cols:
    plt.figure(figsize=(6,4))
    sns.histplot(df[col], kde=True)
    plt.title(f"Distribution of {col}")
    plt.xlabel(col)
    plt.ylabel("Frequency")
    plt.tight_layout()
    plt.show()

"""5. Bivariate analysis: features vs placement

"""

# =====================================================
# SECTION 5: BIVARIATE ANALYSIS (FEATURES vs PLACED)
# =====================================================

# ------------------------------
# 5.1 CATEGORICAL vs PLACED
# ------------------------------

# For each categorical feature, compute the mean of 'placed' (i.e., placement rate)
# across each category to see which categories perform better.
for col in categorical_cols:
    plt.figure(figsize=(6,4))
    rate = df.groupby(col)["placed"].mean().sort_values(ascending=False)
    sns.barplot(x=rate.index, y=rate.values)
    plt.title(f"Placement Rate by {col}")
    plt.ylabel("Mean placed (probability)")
    plt.xticks(rotation=45)
    plt.tight_layout()
    plt.show()

    # Print the exact rates for reporting.
    print(f"\nPlacement rate by {col}:")
    print(rate.round(3))

# ------------------------------
# 5.2 NUMERIC vs PLACED
# ------------------------------

# Boxplots show how the distribution of each numeric feature differs
# between placed (1) and not placed (0) students.
for col in numeric_cols:
    if col == "placed":
        continue
    plt.figure(figsize=(6,4))
    sns.boxplot(x="placed", y=col, data=df)
    plt.title(f"{col} by Placement Status")
    plt.xlabel("Placed (0/1)")
    plt.ylabel(col)
    plt.tight_layout()
    plt.show()

# Compare mean values of numeric features across placement classes.
group_stats = df.groupby("placed")[numeric_cols].mean().T
group_stats

"""6. Correlation analysis and feature relationships"""

# ==========================================
# SECTION 6: CORRELATION & RELATIONSHIPS
# ==========================================

# Compute correlation matrix for numeric variables.
corr = df[numeric_cols].corr()

# Heatmap to visualize correlations (including with 'placed').
plt.figure(figsize=(8,6))
sns.heatmap(corr, annot=True, fmt=".2f", cmap="coolwarm", vmin=-1, vmax=1)
plt.title("Correlation Matrix (Numeric Features)")
plt.tight_layout()
plt.show()

# Sort features by correlation with the target 'placed'.
target_corr = corr["placed"].sort_values(ascending=False)
print("Correlation with 'placed':\n", target_corr)

# Optional: Pairplot for key numeric features together with the target.
key_features = ["cgpa", "active_backlogs", "internships", "projects",
                "aptitude_score", "communication_score", "placed"]

sns.pairplot(df[key_features], hue="placed", diag_kind="kde")
plt.suptitle("Pairwise Relationships (Key Features)", y=1.02)
plt.show()

"""7. Encoding readiness (for later XGBoost)"""

# ==========================================
# SECTION 7: ENCODING READINESS CHECK
# ==========================================

# Check how many unique categories each categorical column has.
for col in categorical_cols:
    print(f"{col}: {df[col].nunique()} unique values")

# Create a copy and demonstrate one-hot encoding for model readiness.
df_encoded = df.copy()

# One-hot encode department and skill_stack.
df_encoded = pd.get_dummies(
    df_encoded,
    columns=["department", "skill_stack"],
    drop_first=True  # avoid dummy variable trap
)

print("Shape after encoding:", df_encoded.shape)
df_encoded.head()

"""## 8. Conclusion

- The dataset provides a comprehensive view of student academic performance, technical skills, internships, projects, and aptitude/communication scores, along with final placement status, enabling robust analysis for placement prediction.
- Exploratory analysis indicates that stronger academic indicators (such as higher CGPA), better aptitude and communication scores, and richer practical exposure through internships and projects generally align with higher placement probabilities.
- Certain departments and skill stacks appear more frequently in the placed group, suggesting that both domain choice and technology stack play an important role in employability.
- The data is structurally clean with manageable levels of missing values and reasonable feature distributions, making it suitable for building an XGBoost-based placement prediction model with appropriate preprocessing and encoding.

"""