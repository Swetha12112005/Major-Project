# -*- coding: utf-8 -*-
"""Student Placements.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mNU9Apj411TPd-HHOjTbNcwC_h7Zj2kV
"""

pip install xgboost

pip install xgboost

import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report

# ---------------------------------------------------------
# 1. DATA GENERATION (Same as before)
# ---------------------------------------------------------
np.random.seed(42)
n_samples = 1000

data = {
    'cgpa': np.random.uniform(5.0, 10.0, n_samples),
    'internships': np.random.randint(0, 4, n_samples),
    'projects': np.random.randint(0, 10, n_samples),
    'aptitude_score': np.random.randint(40, 100, n_samples),
    'communication_score': np.random.randint(1, 10, n_samples),
    'active_backlogs': np.random.randint(0, 5, n_samples),
    'dept': np.random.choice(['CSE', 'ECE', 'EEE', 'MECH'], n_samples),
    'skill_level': np.random.choice(['Low', 'Medium', 'High'], n_samples)
}

df = pd.DataFrame(data)

# Logic for placement
def determine_placement(row):
    score = (row['cgpa'] * 10) + (row['internships'] * 15) + (row['projects'] * 2) + \
            (row['aptitude_score'] * 0.3) + (row['communication_score'] * 2) - (row['active_backlogs'] * 15)
    if row['dept'] == 'CSE': score += 5
    if row['skill_level'] == 'High': score += 10
    elif row['skill_level'] == 'Medium': score += 5
    return 1 if score > 75 else 0

df['placement_status'] = df.apply(determine_placement, axis=1)

# ---------------------------------------------------------
# 2. ADVANCED FEATURE ENGINEERING (Interaction Features)
# ---------------------------------------------------------
# 1. Previous Features
df['cgpa_band'] = pd.cut(df['cgpa'], bins=[0, 6, 7.5, 10], labels=['Low', 'Medium', 'High'])
df['employability_score'] = (df['cgpa'] * 10) + (df['internships'] * 15) + (df['projects'] * 2) + \
                             (df['aptitude_score'] * 0.5) + (df['communication_score'] * 5) - (df['active_backlogs'] * 20)

# 2. NEW: Interaction Features (The Secret Sauce)
# Idea: A student with High CGPA AND Internships is better than the sum of their parts.
df['cgpa_x_internship'] = df['cgpa'] * df['internships']
df['aptitude_x_projects'] = df['aptitude_score'] * df['projects']
df['soft_skill_total'] = df['communication_score'] * (1 - df['active_backlogs']) # Communication drops if backlogs exist

X = df.drop(['placement_status'], axis=1)
y = df['placement_status']

categorical_cols = ['dept', 'skill_level', 'cgpa_band']
numerical_cols = ['cgpa', 'internships', 'projects', 'aptitude_score', 'communication_score',
                  'active_backlogs', 'employability_score', 'cgpa_x_internship',
                  'aptitude_x_projects', 'soft_skill_total']

preprocessor = ColumnTransformer(
    transformers=[
        ('num', StandardScaler(), numerical_cols),
        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)
    ]
)

# ---------------------------------------------------------
# 3. MODEL: XGBoost (Gradient Boosting)
# ---------------------------------------------------------
xgb_model = xgb.XGBClassifier(
    objective='binary:logistic',
    eval_metric='logloss',
    random_state=42,
    use_label_encoder=False
)

pipeline = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', xgb_model)
])

# ---------------------------------------------------------
# 4. AUTOMATED HYPERPARAMETER TUNING (GridSearchCV)
# ---------------------------------------------------------
print("‚è≥ Starting GridSearch for Optimal Parameters (This might take 1-2 mins)...")

# Define a parameter grid to search
param_grid = {
    'classifier__n_estimators': [300, 500],        # Number of trees
    'classifier__learning_rate': [0.05, 0.1],      # Step size shrinkage
    'classifier__max_depth': [4, 6],               # Depth of trees
    'classifier__subsample': [0.8, 1.0],           # Fraction of samples used per tree
    'classifier__colsample_bytree': [0.8, 1.0]     # Fraction of features used per tree
}

cv_strategy = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

grid_search = GridSearchCV(
    pipeline,
    param_grid,
    cv=cv_strategy,
    scoring='accuracy',
    n_jobs=-1, # Use all processors
    verbose=1
)

# Fit GridSearch
grid_search.fit(X, y)

# ---------------------------------------------------------
# 5. RESULTS
# ---------------------------------------------------------
print("\n" + "="*50)
print(f"üèÜ BEST ACCURACY FOUND: {grid_search.best_score_ * 100:.2f}%")
print("="*50)
print(f"Best Parameters: {grid_search.best_params_}")

# Final Evaluation on a hold-out test set (to prove it's not overfitting)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Use the best estimator found by GridSearch
best_model = grid_search.best_estimator_
best_model.fit(X_train, y_train)
y_pred = best_model.predict(X_test)

print(f"\nüìä Final Hold-Out Test Accuracy: {accuracy_score(y_test, y_pred) * 100:.2f}%")
print("\n--- Detailed Report ---")
print(classification_report(y_test, y_pred))

# Feature Importance Plotting
import matplotlib.pyplot as plt
feature_names = (numerical_cols +
                 list(best_model.named_steps['preprocessor']
                      .named_transformers_['cat']
                      .get_feature_names_out(categorical_cols)))

importances = best_model.named_steps['classifier'].feature_importances_
indices = np.argsort(importances)[::-1]

# Plot top 10 features
plt.figure(figsize=(10, 6))
plt.title("Top 10 Important Features (XGBoost)")
plt.bar(range(10), importances[indices[:10]], align="center")
plt.xticks(range(10), [feature_names[i] for i in indices[:10]], rotation=45, ha='right')
plt.tight_layout()
plt.show()

